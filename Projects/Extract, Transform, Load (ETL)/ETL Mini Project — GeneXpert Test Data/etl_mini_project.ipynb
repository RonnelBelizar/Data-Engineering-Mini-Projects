{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf77a232",
   "metadata": {},
   "source": [
    "# ETL Mini Project: GeneXpert Data Pipeline\n",
    "\n",
    "This notebook simulates a real-world ETL process using **mock data** generated by the GeneXpert machine, which I’ve been handling in my current work.\n",
    "\n",
    "The project demonstrates:\n",
    "\n",
    "1. **Extracting** data from multiple file types (`CSV`, `XLSX`, `JSON`)\n",
    "2. **Transforming** it (cleaning, standardizing, error handling)\n",
    "3. **Loading** the cleaned dataset into a CSV file\n",
    "\n",
    "All steps are logged in `etl_log.txt` for tracking and auditing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e54fb",
   "metadata": {},
   "source": [
    "# Sample Raw Data\n",
    "\n",
    "| Patient_ID | Patient_Code | Test_Type | Date_Tested | Status   | Result       |\n",
    "|------------|--------------|-----------|------------|-----------|------------  |\n",
    "| P1000      | GX-337       | HIV       | 2025-09-09 | Error     | 4017         |\n",
    "| P1001      | GX-603       | HIV       | 2025-09-13 | Pending   | Invalid      |\n",
    "| P1002      | GX-950       | MTB       | 2025-09-06 | Pending   | Not Detected |\n",
    "| P1003      | GX-585       | CT_NG     | 2025-08-20 | Error     | 4017         |\n",
    "| P1004      | GX-371       | MTB       | 2025-09-07 | Pending   | Not Detected |\n",
    "\n",
    "## Note:\n",
    "    - Contains duplicates across all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5624aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "etl_log = \"etl_log.txt\"\n",
    "transformed_data_file = \"clean_genexpert_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0524ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Logging Function for each proccesses' timestamp\n",
    "\n",
    "def logging(message):\n",
    "    timestamp_format = '[%Y-%m-%d %H:%M:%S]'\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(etl_log, \"a\") as f:\n",
    "        f.write(f\"{timestamp} INFO: {message}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b284ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Extraction Functions for each file Types\n",
    "\n",
    "def extract_xlsx(xlsx):\n",
    "    extracted_xlsx = pd.read_excel(xlsx)\n",
    "    file_name = os.path.splitext(os.path.basename(xlsx))[0]\n",
    "    extracted_xlsx[\"Source_File\"] = file_name\n",
    "    logging(f\"Extracted {file_name}.xlsx file\")\n",
    "    return extracted_xlsx\n",
    "\n",
    "def extract_csv(csv):\n",
    "    extracted_csv = pd.read_csv(csv)\n",
    "    file_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "    extracted_csv[\"Source_File\"] = file_name\n",
    "    logging(f\"Extracted {file_name}.csv file\")\n",
    "    return extracted_csv\n",
    "\n",
    "def extract_json(json_file):\n",
    "    extracted_json_file = pd.read_json(json_file)\n",
    "    file_name = os.path.splitext(os.path.basename(json_file))[0]\n",
    "    extracted_json_file[\"Source_File\"] = file_name\n",
    "    logging(f\"Extracted {file_name}.json file\")\n",
    "    return extracted_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a8c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Combine Data Function to concatenate all extracted data (CSV/XLSX/JSON)\n",
    "\n",
    "def extract_and_combine_dataframes():\n",
    "    look_xlsx = glob.glob(r\"raw_data\\\\*genexpert_data*.xlsx*\")\n",
    "    look_csv = glob.glob(r\"raw_data\\\\*genexpert_data*.csv*\")\n",
    "    look_json = glob.glob(r\"raw_data\\\\*genexpert_data*.json*\")\n",
    "\n",
    "    final_dfs = []\n",
    "    for xlsx in look_xlsx:\n",
    "        final_dfs.append(extract_xlsx(xlsx))\n",
    "    for csv in look_csv:\n",
    "        final_dfs.append(extract_csv(csv))\n",
    "    for json_file in look_json:\n",
    "        final_dfs.append(extract_json(json_file))\n",
    "\n",
    "    combined_data = pd.concat(final_dfs, ignore_index=True)\n",
    "    logging(\"Extracted XLSX/CSV/JSON files\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149aca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Transform Function to transform the concatenated DataFrame into desired formats\n",
    "\n",
    "def transform(combined_files):\n",
    "    combined_files.columns = combined_files.columns.str.strip().str.lower().str.replace(\"_\", \" \")\n",
    "    logging(\"Standardized Column Names\")\n",
    "    \n",
    "    combined_files = combined_files.drop_duplicates(subset=[\"patient code\", \"test type\", \"result\"])\n",
    "    logging(\"Dropped duplicates based on 'patient code', 'test type', 'result'\")\n",
    "    \n",
    "    combined_files[\"result\"] = combined_files[\"result\"].replace([5007, 4017, 2008, \"5007\", \"4017\", \"2008\"], \"Error\")\n",
    "    logging(\"Replaced error codes as 'Error'\")\n",
    "    \n",
    "    combined_files[\"date tested\"] = pd.to_datetime(combined_files[\"date tested\"])\n",
    "    logging(\"Converted 'date tested' to datetime format\")\n",
    "    \n",
    "    combined_files.sort_values(by=\"date tested\", ascending=False, inplace=True)\n",
    "    logging(\"Sorted data by date\")\n",
    "    \n",
    "    return combined_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9423b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Load Function for file saving\n",
    "\n",
    "def loading(transformed):\n",
    "    transformed.to_csv(transformed_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Extract, Transform, Load (ETL)\n",
    "\n",
    "logging(\"ETL Started...\")\n",
    "\n",
    "logging(\"Data Extraction started\")\n",
    "extracted_data = extract_and_combine_dataframes()\n",
    "logging(\"Data Extraction completed\")\n",
    "\n",
    "logging(\"Data Transformation started\")\n",
    "transformed_data = transform(extracted_data)\n",
    "logging(\"Data Transformation completed\")\n",
    "\n",
    "logging(\"Saving Transformed Data\")\n",
    "loading(transformed_data)\n",
    "logging(\"Saving Successful\")\n",
    "\n",
    "with open(etl_log, \"r\") as file:\n",
    "    count_log_rows = len(file.readlines())\n",
    "\n",
    "logging(\"Logging Completed...\")\n",
    "with open(etl_log, \"a\") as file:\n",
    "    file.write(f\"\\nTotal Logs: {count_log_rows}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382552a",
   "metadata": {},
   "source": [
    "## Transformed GeneXpert Data\n",
    "\n",
    "This table shows the transformed mock data:\n",
    "\n",
    "- Combined from multiple file types (CSV, XLSX, JSON)\n",
    "- Standardized column names\n",
    "- Duplicates removed based on `patient code`, `test type`, and `result`\n",
    "- Error codes (5007, 4017, 2008) replaced with \"Error\"\n",
    "- `date tested` converted to proper datetime format\n",
    "- Sorted by `date tested` in descending order\n",
    "\n",
    "| Patient_ID | Patient_Code | Test_Type | Date_Tested | Status   | Result       | Source_File      |\n",
    "|------------|--------------|-----------|------------|-----------|----------    |------------------|\n",
    "| P1013      | GX-349       | MTB       | 03/10/2025 | Error     | Error        | genexpert_data_4 |\n",
    "| P1013      | GX-349       | MTB       | 03/10/2025 | Error     | Error        | genexpert_data_1 |\n",
    "| P1005      | GX-731       | CT_NG     | 01/10/2025 | Error     | Error        | genexpert_data_5 |\n",
    "| P1010      | GX-112       | HIV       | 01/10/2025 | Error     | Error        | genexpert_data_5 |\n",
    "| P1002      | GX-975       | CT_NG     | 01/10/2025 | Pending   | Detected     | genexpert_data_2 |\n",
    "| P1005      | GX-731       | CT_NG     | 01/10/2025 | Error     | Error        | genexpert_data_2 |\n",
    "| P1010      | GX-112       | HIV       | 01/10/2025 | Error     | Error        | genexpert_data_2 |\n",
    "| P1006      | GX-756       | MTB       | 01/10/2025 | Completed | Not Detected | genexpert_data_3 |\n",
    "| P1005      | GX-673       | CT_NG     | 01/10/2025 | Completed | Invalid      | genexpert_data_1 |\n",
    "| P1001      | GX-238       | CT_NG     | 30/09/2025 | Pending   | Invalid      | genexpert_data_2 |\n",
    "| P1015      | GX-611       | HIV       | 27/09/2025 | Pending   | Invalid      | genexpert_data_3 |\n",
    "| P1007      | GX-816       | HIV       | 26/09/2025 | Pending   | Detected     | genexpert_data_1 |\n",
    "| P1014      | GX-197       | HIV       | 24/09/2025 | Pending   | Detected     | genexpert_data_3 |\n",
    "| P1012      | GX-669       | HIV       | 23/09/2025 | Error     | Error        | genexpert_data_5 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39614755",
   "metadata": {},
   "source": [
    "## ETL Process Log\n",
    "\n",
    "The following log captures the step-by-step execution of the ETL process for the mock GeneXpert data. This includes data extraction, transformation, and saving of the cleaned dataset.\n",
    "\n",
    "```\n",
    "[2025-10-05 00:44:46] INFO: ETL Started...\n",
    "[2025-10-05 00:44:46] INFO: Data Extraction started\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_1.xlsx file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_2.xlsx file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_3.xlsx file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_4.csv file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_5.csv file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_6.csv file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_7.json file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_8.json file\n",
    "[2025-10-05 00:44:47] INFO: Extracted genexpert_data_9.json file\n",
    "[2025-10-05 00:44:47] INFO: Extracted XLSX/CSV/JSON files\n",
    "[2025-10-05 00:44:47] INFO: Data Extraction completed\n",
    "[2025-10-05 00:44:47] INFO: Data Transformation started\n",
    "[2025-10-05 00:44:47] INFO: Standardized Column Names\n",
    "[2025-10-05 00:44:47] INFO: Dropping duplicates based on 'patient code', 'test type', 'result'\n",
    "[2025-10-05 00:44:47] INFO: Replaced error codes as 'Error'\n",
    "[2025-10-05 00:44:47] INFO: Coverted 'date tested' to datetime format\n",
    "[2025-10-05 00:44:47] INFO: Sorted data by date\n",
    "[2025-10-05 00:44:47] INFO: Data Transformation completed\n",
    "[2025-10-05 00:44:47] INFO: Saving Transformed Data\n",
    "[2025-10-05 00:44:47] INFO: Saving Successful\n",
    "[2025-10-05 00:44:47] INFO: Logging Completed...\n",
    "\n",
    "Total Logs: 22\n",
    "```\n",
    "\n",
    "**Note:**  \n",
    "This log provides a complete trace of the ETL process, including extraction of multiple file formats, data cleaning, transformation, and saving the final dataset. Each step is timestamped for tracking and debugging purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d7e7e",
   "metadata": {},
   "source": [
    "## Accessing Data\n",
    "\n",
    "You can access both the **raw data files** and the **cleaned dataset** used in this ETL mini-project:\n",
    "\n",
    "- **Raw data files:** Located in the `raw_data/` folder. These are mock data files generated from GeneXpert machines I’ve been handling in my current work. They include XLSX, CSV, and JSON formats.\n",
    "- **Cleaned dataset:** The final transformed and deduplicated data is saved as `clean_genexpert_data.csv`. This file has standardized column names, cleaned results, and sorted date-tested values.\n",
    "\n",
    "> The cleaned dataset is ready for further analysis, visualization, or reporting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
