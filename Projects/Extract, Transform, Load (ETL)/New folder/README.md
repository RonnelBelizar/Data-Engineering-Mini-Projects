# ğŸ¦ ETL Project: Largest Banks Data Pipeline

### ğŸ“˜ Project Overview
This project demonstrates a simple **ETL (Extract, Transform, Load)** workflow using Python.  
It collects data from a public web source â€” *Wikipediaâ€™s List of Largest Banks* â€” and transforms it into a clean, structured format for analysis and storage.

The goal of this project is to show how automated data collection and transformation can make large financial datasets easier to analyze and integrate into a database system.

---

### ğŸ§© What This Project Does
1. **Extract:**  
   - Scrapes the â€œList of Largest Banksâ€ table from Wikipedia using `requests` and `BeautifulSoup`.

2. **Transform:**  
   - Cleans and standardizes column names.  
   - Converts USD-based market capitalization values into other currencies (GBP, EUR, INR) using exchange rates from a CSV file.  
   - Rounds results for easier readability and consistency.

3. **Load:**  
   - Exports the processed dataset into a CSV file for local storage.  
   - Loads the same dataset into a SQLite database (`Banks.db`) for easy querying and reporting.  
   - Runs basic SQL queries to verify data integrity.

---

### ğŸ› ï¸ Tools & Libraries Used
- **Python 3**  
- **pandas** â€“ for data manipulation  
- **BeautifulSoup (bs4)** â€“ for web scraping  
- **requests** â€“ for fetching web data  
- **SQLite3** â€“ for lightweight database operations  
- **NumPy** â€“ for numerical operations  
- **datetime** â€“ for logging process stages  

---

### ğŸ“„ Output Files
- `Largest_banks_data.csv` â€“ final cleaned dataset  
- `Banks.db` â€“ database containing the transformed table  
- `code_log.txt` â€“ process log with timestamps  

---

### ğŸ§  Key Takeaways
This project highlights:
- End-to-end data automation from extraction to database loading  
- Practical use of Python libraries for real-world ETL tasks  
- Data transformation with currency conversions  
- Process logging and error handling for maintainability  

---

### ğŸ’¼ Why It Matters
Automating data pipelines like this reduces manual effort, ensures data consistency, and prepares information for further analysis â€” valuable skills for roles in **data engineering, analytics, and automation**.

---

### ğŸ‘¨â€ğŸ’» Author
**Ronnel Belizar**  
Electronics & Biomedical Engineer transitioning into Data Engineering  
ğŸ“ Passionate about healthcare data, analytics, and automation  

---

