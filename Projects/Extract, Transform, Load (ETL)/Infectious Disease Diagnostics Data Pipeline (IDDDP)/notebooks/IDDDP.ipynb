{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a0899a",
   "metadata": {},
   "source": [
    "# Infectious Disease Data ETL Pipeline\n",
    "\n",
    "This project demonstrates a complete **ETL (Extract, Transform, Load)** pipeline that processes and stores healthcare-related data for infectious disease tracking.  \n",
    "The goal is to simulate how hospital or laboratory systems consolidate patient information, lab test results, and device maintenance logs into a structured PostgreSQL database for analysis and reporting.\n",
    "\n",
    "**Note:** All datasets used in this project are **mock data** created purely for demonstration and learning purposes. They do not represent any real individuals or healthcare records.\n",
    "\n",
    "### Objectives\n",
    "- Automate the ingestion of raw data from multiple file types (`.json`, `.csv`, `.xlsx`)\n",
    "- Clean and standardize inconsistent data\n",
    "- Load processed data into relational database tables (`patients`, `lab_results`, `maintenance_logs`)\n",
    "- Log each ETL step for tracking and error handling\n",
    "\n",
    "### Tools & Technologies\n",
    "- **Python** â€” scripting language for automation  \n",
    "- **Pandas** â€” data manipulation and cleaning  \n",
    "- **Glob** â€” local file discovery and directory handling  \n",
    "- **SQLAlchemy** â€” PostgreSQL database connectivity  \n",
    "- **PostgreSQL** â€” target database  \n",
    "- **Jupyter Notebook** â€” workflow presentation and documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96910108",
   "metadata": {},
   "source": [
    "### SQL (Database Creation)\n",
    "\n",
    "```sql\n",
    "\n",
    "-- Create the database\n",
    "\n",
    "CREATE DATABASE infectious_disease_db;\n",
    "\n",
    "-- Inside infectious_disease_db, create the following tables:\n",
    "\n",
    "CREATE TABLE patients(\n",
    "\tpatient_id VARCHAR(10) PRIMARY KEY,\n",
    "\tname VARCHAR(50) NULL,\n",
    "\tage INT NULL,\n",
    "\tgender VARCHAR(10) NULL,\n",
    "\tcity VARCHAR(50) NULL,\n",
    "\tcontact VARCHAR(15) NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE lab_results(\n",
    "\tresults_id SERIAL PRIMARY KEY,\n",
    "\tpatient_id VARCHAR(10) REFERENCES patients(patient_id),\n",
    "\ttest_type VARCHAR(10) NULL,\n",
    "\tresult VARCHAR(10) NULL,\n",
    "\tct_value NUMERIC(3,1) NULL,\n",
    "\ttest_date DATE NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE maintenance_logs(\n",
    "\tmaintenance_id SERIAL PRIMARY KEY,\n",
    "\tdevice_id VARCHAR(10) NULL,\n",
    "\tservice_date DATE NULL,\n",
    "\tengineer VARCHAR(25) NULL,\n",
    "\tstatus VARCHAR(15) NULL\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead2803",
   "metadata": {},
   "source": [
    "### ETL Pipeline Overview\n",
    "\n",
    "Now that the **`infectious_disease_db`** database and its tables have been created in PostgreSQL, the next step is to **build and execute the ETL (Extract, Transform, Load) pipeline**.  \n",
    "\n",
    "The following Python scripts use **pandas** for data handling and **SQLAlchemy** for connecting to the PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf9c2d",
   "metadata": {},
   "source": [
    "### Library Imports & Configuration\n",
    "\n",
    "To begin the ETL process, import the necessary Python libraries and set up configuration variables, such as file directories, table names, and database connection details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "raw_data_dir_ = \"./infectious_disease_pipeline_data/\"\n",
    "patients_table = \"patients\"\n",
    "lab_results_table = \"lab_results\"\n",
    "maintenance_logs_table = \"maintenance_logs\"\n",
    "etl_logs = \"./logs/etl_logs.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebea04",
   "metadata": {},
   "source": [
    "### Logging Function\n",
    "\n",
    "The `log_progress()` function records timestamps and progress messages throughout the ETL pipeline.  \n",
    "Logs are saved in a local text file (`etl_logs.txt`) to help track the flow and identify errors during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%m-%d %H:%M:%S'\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(etl_logs, \"a\") as file:\n",
    "        file.write(f\"{timestamp}: {message}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a33698",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "This step reads data from multiple file formats stored in the `infectious_disease_pipeline_data` directory:\n",
    "- **JSON files** for patient information  \n",
    "- **CSV files** for laboratory results  \n",
    "- **Excel files** for device maintenance logs  \n",
    "\n",
    "All extracted files are concatenated into Pandas DataFrames for further transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_info():\n",
    "    files = glob.glob(f\"{raw_data_dir_}*.json\")\n",
    "    json_files = []\n",
    "    for data in files:\n",
    "        df = pd.read_json(data)\n",
    "        json_files.append(df)\n",
    "    df_total = pd.concat(json_files, ignore_index=True)\n",
    "    return df_total\n",
    "\n",
    "\n",
    "def extract_lab_results():\n",
    "    files = glob.glob(f\"{raw_data_dir_}*.csv\")\n",
    "    csv_files = []\n",
    "    for data in files:\n",
    "        df = pd.read_csv(data)\n",
    "        csv_files.append(df)\n",
    "    df_total = pd.concat(csv_files, ignore_index=True)\n",
    "    return df_total\n",
    "\n",
    "\n",
    "def extract_device_maintenance():\n",
    "    files = glob.glob(f\"{raw_data_dir_}*.xlsx\")\n",
    "    xlsx_files = []\n",
    "    for data in files:\n",
    "        df = pd.read_excel(data)\n",
    "        xlsx_files.append(df)\n",
    "    df_total = pd.concat(xlsx_files, ignore_index=True)\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee2bca",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "After extraction, the data is cleaned and standardized:\n",
    "- Removing unwanted characters  \n",
    "- Converting text to proper casing  \n",
    "- Handling missing or invalid values  \n",
    "- Removing duplicate entries  \n",
    "\n",
    "This ensures that only consistent and validated data is loaded into the PostgreSQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e995d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_patient_info(df):\n",
    "\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str).str.strip().str.upper()\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce').astype('Int64')\n",
    "    df[\"gender\"] = df[\"gender\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df[\"city\"] = df[\"city\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df[\"contact\"] = pd.to_numeric(\n",
    "        df[\"contact\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df.drop_duplicates(subset=['name', 'contact'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_lab_results(df):\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str).str.strip().str.upper()\n",
    "    df[\"test_type\"] = df[\"test_type\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df[\"result\"] = df[\"result\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df[\"ct_value\"] = pd.to_numeric(df['ct_value'], errors='coerce')\n",
    "    df[\"test_date\"] = pd.to_datetime(df[\"test_date\"], errors='coerce')\n",
    "    df.drop_duplicates(subset=[\"patient_id\", \"test_type\",\n",
    "                       \"result\", \"ct_value\", \"test_date\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_device_maintenance(df):\n",
    "    df[\"device_id\"] = df[\"device_id\"].astype(str).str.strip().str.upper()\n",
    "    df[\"service_date\"] = pd.to_datetime(df[\"service_date\"])\n",
    "    df[\"engineer\"] = df[\"engineer\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df[\"status\"] = df[\"status\"].astype(str).str.strip().replace(\n",
    "        r'[^A-Za-z\\s]', '', regex=True).str.title()\n",
    "    df.drop_duplicates(\n",
    "        subset=[\"device_id\", \"service_date\", \"engineer\", \"status\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26115192",
   "metadata": {},
   "source": [
    "### Database Connection and Data Loading\n",
    "\n",
    "After transformation, the cleaned DataFrames are inserted into the PostgreSQL database using SQLAlchemy.  \n",
    "Each DataFrame corresponds to a specific table (`patients`, `lab_results`, `maintenance_logs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_patient_info(df_patients, df_lab_results, df_device_maintenance):\n",
    "    df_patients.to_sql(patients_table, engine, if_exists=\"append\", index=False)\n",
    "    df_lab_results.to_sql(lab_results_table, engine,\n",
    "                          if_exists=\"append\", index=False)\n",
    "    df_device_maintenance.to_sql(\n",
    "        maintenance_logs_table, engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb947e52",
   "metadata": {},
   "source": [
    "### ETL Execution\n",
    "\n",
    "Below is the full ETL execution sequence â€”  \n",
    "1. Create a database connection  \n",
    "2. Extract data  \n",
    "3. Transform it  \n",
    "4. Load into PostgreSQL  \n",
    "5. Log each step for tracking and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_progress('Starting ETL Process')\n",
    "\n",
    "try:\n",
    "    # Creating SQLAlchemy Engine\n",
    "    log_progress('Establishing SQLAlchemy Engine')\n",
    "    engine = create_engine(\n",
    "        \"postgresql+psycopg2://postgres:Madron_91@localhost:5432/infectious_disease_db\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'Error Occured During Engine Creation: {e}')\n",
    "    log_progress(f'Error Occured During Engine Creation: {e}')\n",
    "else:\n",
    "    log_progress('Successfully Created SQLAlchemy Engine')\n",
    "\n",
    "# Extraction Process\n",
    "\n",
    "log_progress('Initializing Extract Process')\n",
    "\n",
    "try:\n",
    "    log_progress('Extracting Patient Info')\n",
    "    extract_patients = extract_patient_info()\n",
    "    log_progress('Successfully Extracted Patient Info')\n",
    "\n",
    "    log_progress('Extracting Lab Results')\n",
    "    extract_lab = extract_lab_results()\n",
    "    log_progress('Successfully Extracted Lab Results')\n",
    "\n",
    "    log_progress('Extracting Device Maintenance')\n",
    "    extract_maintenance = extract_device_maintenance()\n",
    "    log_progress('Successfully Extracted Device Maintenance')\n",
    "except Exception as e:\n",
    "    print(f'Error Occured During Extraction: {e}')\n",
    "    log_progress(f'Error Occured During Extraction: {e}')\n",
    "else:\n",
    "    log_progress('Successfully Extracted Data')\n",
    "\n",
    "# Transformation Process\n",
    "\n",
    "log_progress('Initializing Transformation')\n",
    "\n",
    "try:\n",
    "    log_progress('Transforming Patient Info')\n",
    "    transform_patients = transform_patient_info(extract_patients)\n",
    "    log_progress('Successfully Transformed Patient Info')\n",
    "\n",
    "    log_progress('Transforming Lab Results')\n",
    "    transform_lab = transform_lab_results(extract_lab)\n",
    "    log_progress('Successfully Transformed Lab Results')\n",
    "\n",
    "    log_progress('Transforming Device Maintenance')\n",
    "    transform_maintenance = transform_device_maintenance(extract_maintenance)\n",
    "    log_progress('Successfully Transformed Device Maintenance')\n",
    "except Exception as e:\n",
    "    print(f'Error Occured During Transformation: {e}')\n",
    "    log_progress(f'Error Occured During Transformation: {e}')\n",
    "else:\n",
    "    log_progress('Successfully Transformed DataFrames')\n",
    "\n",
    "# Loading DFs to PostgreSQL\n",
    "\n",
    "log_progress('Loading DataFrames into PostgreSQL DB')\n",
    "\n",
    "try:\n",
    "    loading_patient_info(transform_patients, transform_lab,\n",
    "                         transform_maintenance)\n",
    "except Exception as e:\n",
    "    print(f'Error Occured During Loading: {e}')\n",
    "    log_progress(f'Error Occured During Loading: {e}')\n",
    "else:\n",
    "    log_progress('Successfully Loaded DataFrames')\n",
    "\n",
    "# Closing SQLAlchemy Engine\n",
    "\n",
    "engine.dispose()\n",
    "log_progress('Closed SQLAlchemy Engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54eaf6d",
   "metadata": {},
   "source": [
    "### Verification\n",
    "\n",
    "After the ETL process completes, you can query the PostgreSQL database to confirm that the data has been successfully loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ee389",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT * FROM patients;\n",
    "SELECT * FROM lab_results;\n",
    "SELECT * FROM maintenance_logs;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f584d4",
   "metadata": {},
   "source": [
    "### PostgreSQL Verification Screenshots\n",
    "\n",
    "Below are the verification screenshots showing that the data was successfully loaded into the PostgreSQL database.\n",
    "\n",
    "---\n",
    "\n",
    "#### Patients Table\n",
    "\n",
    "This table shows the patient information successfully inserted into the `patients` table after the ETL process.\n",
    "\n",
    "![Patients Table](./screenshots/patients_table.png)\n",
    "\n",
    "*Data verification successful â€” all patient records loaded.*\n",
    "\n",
    "---\n",
    "\n",
    "#### Lab Results Table\n",
    "\n",
    "This screenshot confirms that laboratory test data was correctly inserted into the `lab_results` table.\n",
    "\n",
    "![Lab Results Table](./screenshots/lab_results_table.png)\n",
    "\n",
    "*Data verification successful â€” all lab results loaded.*\n",
    "\n",
    "---\n",
    "\n",
    "#### Maintenance Logs Table\n",
    "\n",
    "This screenshot verifies that device maintenance records were properly inserted into the `maintenance_logs` table.\n",
    "\n",
    "![Maintenance Logs Table](./screenshots/maintenance_logs_table.png)\n",
    "\n",
    "*Data verification successful â€” all maintenance logs loaded.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183aeaf",
   "metadata": {},
   "source": [
    "### ðŸ§¾ ETL Execution Logs\n",
    "\n",
    "These are the actual logs automatically generated during the ETL process.  \n",
    "They confirm that each stage â€” extraction, transformation, and loading â€” executed successfully without any errors.\n",
    "\n",
    "**Log File Location:** `./logs/etl_logs.txt`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸª¶ ETL Log Output\n",
    "\n",
    "```text\n",
    "2025-10-19 22:41:56: Starting ETL Process\n",
    "2025-10-19 22:41:56: Establishing SQLAlchemy Engine\n",
    "2025-10-19 22:41:56: Successfully Created SQLAlchemy Engine\n",
    "2025-10-19 22:41:56: Initializing Extract Process\n",
    "2025-10-19 22:41:56: Extracting Patient Info\n",
    "2025-10-19 22:41:56: Successfully Extracted Patient Info\n",
    "2025-10-19 22:41:56: Extracting Lab Results\n",
    "2025-10-19 22:41:56: Successfully Extracted Lab Results\n",
    "2025-10-19 22:41:56: Extracting Device Maintenance\n",
    "2025-10-19 22:41:56: Successfully Extracted Device Maintenance\n",
    "2025-10-19 22:41:56: Successfully Extracted Data\n",
    "2025-10-19 22:41:56: Initializing Transformation\n",
    "2025-10-19 22:41:56: Transforming Patient Info\n",
    "2025-10-19 22:41:56: Successfully Transformed Patient Info\n",
    "2025-10-19 22:41:56: Transforming Lab Results\n",
    "2025-10-19 22:41:56: Successfully Transformed Lab Results\n",
    "2025-10-19 22:41:56: Transforming Device Maintenance\n",
    "2025-10-19 22:41:56: Successfully Transformed Device Maintenance\n",
    "2025-10-19 22:41:56: Successfully Transformed DataFrames\n",
    "2025-10-19 22:41:56: Loading DataFrames into PostgreSQL DB\n",
    "2025-10-19 22:41:56: Successfully Loaded DataFrames\n",
    "2025-10-19 22:41:56: Closed SQLAlchemy Engine\n",
    "```\n",
    "\n",
    "ETL process executed successfully â€” all steps completed without errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbda54c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The ETL pipeline successfully:\n",
    "- Extracted raw data from multiple formats (`.json`, `.csv`, `.xlsx`)  \n",
    "- Transformed and cleaned the datasets for consistency  \n",
    "- Loaded structured data into a PostgreSQL database  \n",
    "\n",
    "This workflow demonstrates how a data engineer can automate data consolidation in a healthcare environment â€” ensuring clean, reliable, and ready-to-analyze data for further use in dashboards, analytics, or machine learning.\n",
    "\n",
    "### Next Steps\n",
    "- Implement data validation checks (e.g., missing values, schema enforcement)\n",
    "- Schedule ETL jobs using **Airflow** or **Cron**\n",
    "- Extend the pipeline with **data visualization** or **analytics dashboards**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
